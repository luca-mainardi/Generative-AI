{
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [],
      "dockerImageVersionId": 30733,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment 2 - Variational Auto-Encoders\n",
        "## Generative AI Models 2024"
      ],
      "metadata": {
        "id": "aJ_pmgxvGur9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Instructions on how to use this notebook:\n",
        "\n",
        "This notebook is hosted on ``Google Colab``. To be able to work on it, you have to create your own copy. Go to *File* and select *Save a copy in Drive*.\n",
        "\n",
        "You can also avoid using ``Colab`` entirely, and download the notebook to run it on your own machine. If you choose this, go to *File* and select *Download .ipynb*.\n",
        "\n",
        "The advantage of using **Colab** is that you can use a GPU. You can complete this assignment with a CPU, but it will take a bit longer. Furthermore, we encourage you to train using the GPU not only for faster training, but also to get experience with this setting. This includes moving models and tensors to the GPU and back. This experience is very valuable because for various models and large datasets (like large CNNs for ImageNet, or Transformer models trained on Wikipedia), training on GPU is the only feasible way.\n",
        "\n",
        "The default ``Colab`` runtime does not have a GPU. To change this, go to *Runtime - Change runtime type*, and select *GPU* as the hardware accelerator. The GPU that you get changes according to what resources are available at the time, and its memory can go from a 5GB, to around 18GB if you are lucky. If you are curious, you can run the following in a code cell to check:\n",
        "\n",
        "```sh\n",
        "!nvidia-smi\n",
        "```\n",
        "\n",
        "Note that despite the name, ``Google Colab`` does  not support collaborative work without issues. When two or more people edit the notebook concurrently, only one version will be saved. You can choose to do group programming with one person sharing the screen with the others, or make multiple copies of the notebook to work concurrently.\n",
        "\n",
        "**Submission:** Please bring your (partial) solution to instruction sessions. Then you can discuss it with intructors and your colleagues."
      ],
      "metadata": {
        "id": "mEneMITS2agU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lBgoJIpdLI2Y",
        "outputId": "024a3fd1-196a-4e91-e243-3f5afb87d96e",
        "execution": {
          "iopub.status.busy": "2024-06-07T14:40:17.095018Z",
          "iopub.execute_input": "2024-06-07T14:40:17.095404Z",
          "iopub.status.idle": "2024-06-07T14:40:18.148084Z",
          "shell.execute_reply.started": "2024-06-07T14:40:17.095374Z",
          "shell.execute_reply": "2024-06-07T14:40:18.146897Z"
        },
        "trusted": true
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Jun  9 20:52:45 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   40C    P8               9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction\n",
        "\n",
        "In this assignment, we are going to implement a Variational Auto-Encoder (VAE). A VAE is a likelihood-based deep generative model that consists of a stochastic encoder (a variational posterior over latent variables), a stochastic decoder, and a marginal distribution over latent variables (a.k.a. a prior). The model was originally proposed in two concurrent papers:\n",
        "- [Kingma, D. P., & Welling, M. (2013). Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114.](https://arxiv.org/abs/1312.6114)\n",
        "- [Rezende, Danilo Jimenez, Shakir Mohamed, and Daan Wierstra. \"Stochastic backpropagation and approximate inference in deep generative models.\" International conference on machine learning. PMLR, 2014.](https://proceedings.mlr.press/v32/rezende14.html)\n",
        "\n",
        "You can read more about VAEs in Chapter 4 of the following book:\n",
        "- [Tomczak, J.M., \"Deep Generative Modeling\", Springer, 2022](https://link.springer.com/book/10.1007/978-3-030-93158-2)\n",
        "\n",
        "In particular, the goals of this assignment are the following:\n",
        "\n",
        "- Understand how VAEs are formulated.\n",
        "- Implement components of VAEs using PyTorch.\n",
        "- Train and evaluate your VAE model on image data.\n",
        "\n",
        "This notebook is essential for preparing a report. Moreover, please remember to submit the final notebook together with the report (PDF)."
      ],
      "metadata": {
        "id": "tsdc7fDp40rQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Theory behind VAEs\n",
        "\n",
        "VAEs are latent variable models trained with variational inference. In general, the latent variable models define the following generative process:\n",
        "\\begin{align}\n",
        "1.\\ & \\mathbf{z} \\sim p_{\\lambda}(\\mathbf{z}) \\\\\n",
        "2.\\ & \\mathbf{x} \\sim p_{\\theta}(\\mathbf{x}|\\mathbf{z})\n",
        "\\end{align}\n",
        "\n",
        "In plain words, we assume that for observable data $\\mathbf{x}$, there are some latent (hidden) factors $\\mathbf{z}$. Then, the training objective is log-likelihood function of the following form:\n",
        "$$\n",
        "\\log p_{\\vartheta}(\\mathbf{x})=\\log \\int p_\\theta(\\mathbf{x} \\mid \\mathbf{z}) p_\\lambda(\\mathbf{z}) \\mathrm{d} \\mathbf{z} .\n",
        "$$\n",
        "\n",
        "The problem here is the intractability of the integral if the dependencies between random variables $\\mathbf{x}$ and $\\mathbf{z}$ are non-linear and/or the distributions are non-Gaussian.\n",
        "\n",
        "By introducing variational posteriors $q_{\\phi}(\\mathbf{z}|\\mathbf{x})$, we get the following lower bound (the Evidence Lower Bound, ELBO):\n",
        "$$\n",
        "\\log p_{\\vartheta}(\\mathbf{x}) \\geq \\mathbb{E}_{\\mathbf{z} \\sim q_\\phi(\\mathbf{z} \\mid \\mathbf{x})}\\left[\\log p_\\theta(\\mathbf{x} \\mid \\mathbf{z})\\right]-\\mathrm{KL}\\left(q_\\phi(\\mathbf{z} \\mid \\mathbf{x}) \\| p_\\lambda(\\mathbf{z})\\right) .\n",
        "$$"
      ],
      "metadata": {
        "id": "RvsuVNczG6pP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## IMPORTS"
      ],
      "metadata": {
        "id": "suzhlbWqxtD9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DO NOT REMOVE!\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torchvision\n",
        "from torchvision.datasets import MNIST"
      ],
      "metadata": {
        "id": "BjxkigYLxpB7",
        "execution": {
          "iopub.status.busy": "2024-06-07T14:40:18.150321Z",
          "iopub.execute_input": "2024-06-07T14:40:18.150672Z",
          "iopub.status.idle": "2024-06-07T14:40:18.156542Z",
          "shell.execute_reply.started": "2024-06-07T14:40:18.150633Z",
          "shell.execute_reply": "2024-06-07T14:40:18.155623Z"
        },
        "trusted": true
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DO NOT REMOVE!\n",
        "# Check if GPU is available and determine the device\n",
        "if torch.cuda.is_available():\n",
        "  device = 'cuda'\n",
        "else:\n",
        "  device = 'cpu'\n",
        "\n",
        "print(f'The available device is {device}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cm23hRm6CqGh",
        "outputId": "19315c6a-9cc0-4a33-c204-03941f200f4d",
        "execution": {
          "iopub.status.busy": "2024-06-07T14:40:18.157544Z",
          "iopub.execute_input": "2024-06-07T14:40:18.157848Z",
          "iopub.status.idle": "2024-06-07T14:40:18.168212Z",
          "shell.execute_reply.started": "2024-06-07T14:40:18.157817Z",
          "shell.execute_reply": "2024-06-07T14:40:18.167333Z"
        },
        "trusted": true
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The available device is cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# DO NOT REMOVE! (unless you work locally)\n",
        "# mount drive: WE NEED IT FOR SAVING IMAGES! NECESSARY FOR GOOGLE COLAB!\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OoPb92zNM4UY",
        "outputId": "4f439404-2c01-4041-bf21-dd8997bee52a",
        "execution": {
          "iopub.status.busy": "2024-06-07T14:40:18.180681Z",
          "iopub.execute_input": "2024-06-07T14:40:18.181164Z",
          "iopub.status.idle": "2024-06-07T14:40:18.193596Z",
          "shell.execute_reply.started": "2024-06-07T14:40:18.181104Z",
          "shell.execute_reply": "2024-06-07T14:40:18.192804Z"
        },
        "trusted": true
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# DO NOT REMOVE! (unless you work locally)\n",
        "# PLEASE CHANGE IT TO YOUR OWN GOOGLE DRIVE!\n",
        "images_dir = '/content/gdrive/My Drive/Results/'"
      ],
      "metadata": {
        "id": "eJ1R-9GKxC5Q",
        "execution": {
          "iopub.status.busy": "2024-06-07T14:40:18.194760Z",
          "iopub.execute_input": "2024-06-07T14:40:18.195495Z",
          "iopub.status.idle": "2024-06-07T14:40:18.203472Z",
          "shell.execute_reply.started": "2024-06-07T14:40:18.195460Z",
          "shell.execute_reply": "2024-06-07T14:40:18.202584Z"
        },
        "trusted": true
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Auxiliary functions"
      ],
      "metadata": {
        "id": "I3zs31tOyCmq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us define some useful log-distributions:"
      ],
      "metadata": {
        "id": "DF0agzL7tDHK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DO NOT REMOVE\n",
        "PI = torch.from_numpy(np.asarray(np.pi))\n",
        "EPS = 1.e-5\n",
        "\n",
        "def log_categorical(x, p, num_classes=256, reduction=None):\n",
        "    x_one_hot = F.one_hot(x.long(), num_classes=num_classes)\n",
        "    log_p = x_one_hot * torch.log(torch.clamp(p, EPS, 1. - EPS))\n",
        "    if reduction == 'mean':\n",
        "        return torch.mean(log_p, list(range(1, len(x.shape))))\n",
        "    elif reduction == 'sum':\n",
        "        return torch.sum(log_p, list(range(1, len(x.shape))))\n",
        "    else:\n",
        "        return log_p\n",
        "\n",
        "def log_bernoulli(x, p, reduction=None):\n",
        "    pp = torch.clamp(p, EPS, 1. - EPS)\n",
        "    log_p = x * torch.log(pp) + (1. - x) * torch.log(1. - pp)\n",
        "    if reduction == 'mean':\n",
        "        return torch.mean(log_p, list(range(1, len(x.shape))))\n",
        "    elif reduction == 'sum':\n",
        "        return torch.sum(log_p, list(range(1, len(x.shape))))\n",
        "    else:\n",
        "        return log_p\n",
        "\n",
        "def log_normal_diag(x, mu, log_var, reduction=None):\n",
        "    D = x.shape[1]\n",
        "    log_p = -0.5 * torch.log(2. * PI) - 0.5 * log_var - 0.5 * torch.exp(-log_var) * (x - mu)**2.\n",
        "    if reduction == 'mean':\n",
        "        return torch.mean(torch.sum(log_p, list(range(1, len(x.shape)))))\n",
        "    elif reduction == 'sum':\n",
        "        return torch.sum(torch.sum(log_p, list(range(1, len(x.shape)))))\n",
        "    else:\n",
        "        return log_p\n",
        "\n",
        "\n",
        "def log_standard_normal(x, reduction=None):\n",
        "    D = x.shape[1]\n",
        "    log_p = -0.5 * torch.log(2. * PI) - 0.5 * x**2.\n",
        "    if reduction == 'mean':\n",
        "        return torch.mean(log_p, list(range(1, len(x.shape))))\n",
        "    elif reduction == 'sum':\n",
        "        return torch.sum(log_p, list(range(1, len(x.shape))))\n",
        "    else:\n",
        "        return log_p"
      ],
      "metadata": {
        "id": "LIBNVRNJtHSd",
        "execution": {
          "iopub.status.busy": "2024-06-07T14:40:18.204687Z",
          "iopub.execute_input": "2024-06-07T14:40:18.205220Z",
          "iopub.status.idle": "2024-06-07T14:40:18.220041Z",
          "shell.execute_reply.started": "2024-06-07T14:40:18.205188Z",
          "shell.execute_reply": "2024-06-07T14:40:18.219190Z"
        },
        "trusted": true
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementing VAEs\n",
        "\n",
        "The goal of this assignment is to implement four classes:\n",
        "- `Encoder`: this class implements the encoder (variational posterior), $q_{\\phi}(\\mathbf{z}|\\mathbf{x})$.\n",
        "- `Decoder`: this class implements the decoded (the conditional likelihood), $p_{\\theta}(\\mathbf{x}|\\mathbf{z})$.\n",
        "- `Prior`: this class implements the marginal over latents (the prior), $p_{\\lambda}(\\mathbf{z})$.\n",
        "- `VAE`: this class combines all components."
      ],
      "metadata": {
        "id": "Q2LLOs0kn7iw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Encoder\n",
        "We start with `Encoder`. Please remember that we assume the Gaussian variational posterior with a diagonal covariance matrix."
      ],
      "metadata": {
        "id": "7cXhOwKAzW6Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE GOES IN THIS CELL\n",
        "# NOTE: The class must containt the following functions:\n",
        "# (i) reparameterization\n",
        "# (ii) sample\n",
        "# Moreover, forward must return the log-probability of variational posterior for given x, i.e., log q(z|x)\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, encoder_net):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        # The init of the encoder network.\n",
        "        self.encoder = encoder_net\n",
        "\n",
        "    # The reparameterization trick for Gaussians.\n",
        "    @staticmethod\n",
        "    def reparameterization(mu, log_var):\n",
        "        # The formulat is the following:\n",
        "        # z = mu + std * epsilon\n",
        "        # epsilon ~ Normal(0,1)\n",
        "\n",
        "        # First, we need to get std from log-variance.\n",
        "        std = torch.exp(0.5*log_var)\n",
        "\n",
        "        # Second, we sample epsilon from Normal(0,1).\n",
        "        eps = torch.randn_like(std)\n",
        "\n",
        "        # The final output\n",
        "        return mu + std * eps\n",
        "\n",
        "    # This function implements the output of the encoder network (i.e., parameters of a Gaussian).\n",
        "    def encode(self, x):\n",
        "        # First, we calculate the output of the encoder netowork of size 2M.\n",
        "        h_e = self.encoder(x)\n",
        "        # Second, we must divide the output to the mean and the log-variance.\n",
        "        mu_e, log_var_e = torch.chunk(h_e, 2, dim=1)\n",
        "\n",
        "        return mu_e, log_var_e\n",
        "\n",
        "    # Sampling procedure.\n",
        "    def sample(self, x=None, mu_e=None, log_var_e=None):\n",
        "        #If we don't provide a mean and a log-variance, we must first calcuate it:\n",
        "        if (mu_e is None) and (log_var_e is None):\n",
        "            mu_e, log_var_e = self.encode(x)\n",
        "        # Or the final sample\n",
        "        else:\n",
        "        # Otherwise, we can simply apply the reparameterization trick!\n",
        "            if (mu_e is None) or (log_var_e is None):\n",
        "                raise ValueError('mu and log-var can`t be None!')\n",
        "        z = self.reparameterization(mu_e, log_var_e)\n",
        "        return z\n",
        "\n",
        "    # This function calculates the log-probability that is later used for calculating the ELBO.\n",
        "    def log_prob(self, x=None, mu_e=None, log_var_e=None, z=None):\n",
        "        # If we provide x alone, then we can calculate a corresponsing sample:\n",
        "        if x is not None:\n",
        "            mu_e, log_var_e = self.encode(x)\n",
        "            z = self.sample(mu_e=mu_e, log_var_e=log_var_e)\n",
        "        else:\n",
        "        # Otherwise, we should provide mu, log-var and z!\n",
        "            if (mu_e is None) or (log_var_e is None) or (z is None):\n",
        "                raise ValueError('mu, log-var and z can`t be None!')\n",
        "\n",
        "        return log_normal_diag(z, mu_e, log_var_e)\n",
        "\n",
        "    # PyTorch forward pass: it is either log-probability (by default) or sampling.\n",
        "    def forward(self, x, type='log_prob'):\n",
        "        assert type in ['encode', 'log_prob'], 'Type could be either encode or log_prob'\n",
        "        if type == 'log_prob':\n",
        "            return self.log_prob(x)\n",
        "        else:\n",
        "            return self.sample(x)"
      ],
      "metadata": {
        "id": "MrwQXSuEoFfH",
        "execution": {
          "iopub.status.busy": "2024-06-07T14:40:18.221194Z",
          "iopub.execute_input": "2024-06-07T14:40:18.221444Z",
          "iopub.status.idle": "2024-06-07T14:40:18.235210Z",
          "shell.execute_reply.started": "2024-06-07T14:40:18.221422Z",
          "shell.execute_reply": "2024-06-07T14:40:18.234336Z"
        },
        "trusted": true
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Decoder\n",
        "\n",
        "The decoder is the conditional likelihood, i.e., $p(x|z)$. Please remember that we must decide on the form of the distribution (e.g., Bernoulli, Gaussian, Categorical)."
      ],
      "metadata": {
        "id": "nhNTy5mn0XDT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE GOES IN THIS CELL\n",
        "# NOTE: The class must containt the following function:\n",
        "# (i) sample\n",
        "# Moreover, forward must return the log-probability of the conditional likelihood function for given z, i.e., log p(x|z)\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, decoder_net, distribution='categorical', num_vals=None):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        # The decoder network.\n",
        "        self.decoder = decoder_net\n",
        "        # The distribution used for the decoder (it is categorical by default, as discussed above).\n",
        "        self.distribution = distribution\n",
        "        # The number of possible values. This is important for the categorical distribution.\n",
        "        self.num_vals=num_vals\n",
        "\n",
        "    # This function calculates parameters of the likelihood function p(x|z)\n",
        "    def decode(self, z):\n",
        "        # First, we apply the decoder network.\n",
        "        h_d = self.decoder(z)\n",
        "\n",
        "        # In this example, we use only the categorical distribution...\n",
        "        if self.distribution == 'categorical':\n",
        "            # We save the shapes: batch size\n",
        "            b = h_d.shape[0]\n",
        "            # and the dimensionality of x.\n",
        "            d = h_d.shape[1]//self.num_vals\n",
        "            # Then we reshape to (Batch size, Dimensionality, Number of Values).\n",
        "            h_d = h_d.view(b, d, self.num_vals)\n",
        "            # To get probabilities, we apply softmax.\n",
        "            mu_d = torch.softmax(h_d, 2)\n",
        "            return [mu_d]\n",
        "        # ... however, we also present the Bernoulli distribution. We are nice, aren't we?\n",
        "        elif self.distribution == 'bernoulli':\n",
        "            # In the Bernoulli case, we have x_d \\in {0,1}. Therefore, it is enough to output a single probability,\n",
        "            # because p(x_d=1|z) = \\theta and p(x_d=0|z) = 1 - \\theta\n",
        "            mu_d = torch.sigmoid(h_d)\n",
        "            return [mu_d]\n",
        "\n",
        "        else:\n",
        "            raise ValueError('Either `categorical` or `bernoulli`')\n",
        "\n",
        "    # This function implements sampling from the decoder.\n",
        "    def sample(self, z):\n",
        "        outs = self.decode(z)\n",
        "\n",
        "        if self.distribution == 'categorical':\n",
        "            # We take the output of the decoder\n",
        "            mu_d = outs[0]\n",
        "            # and save shapes (we will need that for reshaping).\n",
        "            b = mu_d.shape[0]\n",
        "            m = mu_d.shape[1]\n",
        "            # Here we use reshaping\n",
        "            mu_d = mu_d.view(mu_d.shape[0], -1, self.num_vals)\n",
        "            p = mu_d.view(-1, self.num_vals)\n",
        "            # Eventually, we sample from the categorical (the built-in PyTorch function).\n",
        "            x_new = torch.multinomial(p, num_samples=1).view(b, m)\n",
        "\n",
        "        elif self.distribution == 'bernoulli':\n",
        "            # In the case of Bernoulli, we don't need any reshaping\n",
        "            mu_d = outs[0]\n",
        "            # and we can use the built-in PyTorch sampler!\n",
        "            x_new = torch.bernoulli(mu_d)\n",
        "\n",
        "        else:\n",
        "            raise ValueError('Either `categorical` or `bernoulli`')\n",
        "\n",
        "        return x_new\n",
        "\n",
        "    # This function calculates the conditional log-likelihood function.\n",
        "    def log_prob(self, x, z):\n",
        "        outs = self.decode(z)\n",
        "\n",
        "        if self.distribution == 'categorical':\n",
        "            mu_d = outs[0]\n",
        "            log_p = log_categorical(x, mu_d, num_classes=self.num_vals, reduction='sum').sum(-1)\n",
        "\n",
        "        elif self.distribution == 'bernoulli':\n",
        "            mu_d = outs[0]\n",
        "            log_p = log_bernoulli(x, mu_d, reduction='sum')\n",
        "\n",
        "        else:\n",
        "            raise ValueError('Either `categorical` or `bernoulli`')\n",
        "\n",
        "        return log_p\n",
        "\n",
        "    # The forward pass is either a log-prob or a sample.\n",
        "    def forward(self, z, x=None, type='log_prob'):\n",
        "        assert type in ['decoder', 'log_prob'], 'Type could be either decode or log_prob'\n",
        "        if type == 'log_prob':\n",
        "            return self.log_prob(x, z)\n",
        "        else:\n",
        "            return self.sample(x)"
      ],
      "metadata": {
        "id": "9vTmKHwrpUVa",
        "execution": {
          "iopub.status.busy": "2024-06-07T14:40:18.236303Z",
          "iopub.execute_input": "2024-06-07T14:40:18.236599Z",
          "iopub.status.idle": "2024-06-07T14:40:18.252505Z",
          "shell.execute_reply.started": "2024-06-07T14:40:18.236554Z",
          "shell.execute_reply": "2024-06-07T14:40:18.251678Z"
        },
        "trusted": true
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prior\n",
        "\n",
        "The prior is the marginal distribution over latent variables, i.e., $p(z)$. It plays a crucial role in the generative process and also in synthesizing images of a better quality.\n",
        "\n",
        "In this assignment, you are asked to implement a prior that is learnable (e.g., parameterized by a neural network). If you decide to implement the standard Gaussian prior only, then please be aware that you will not get any points.\n",
        "\n",
        "For the learnable prior you can choose the **Mixture of Gaussians**."
      ],
      "metadata": {
        "id": "CLIEwIiw00op"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE GOES IN THIS CELL\n",
        "# NOTES:\n",
        "# (i) Implementing the standard Gaussian prior does not give you any points!\n",
        "# (ii) The function \"sample\" must be implemented.\n",
        "# (iii) The function \"forward\" must return the log-probability, i.e., log p(z)\n",
        "\n",
        "class MoGPrior(nn.Module):\n",
        "    def __init__(self, L, num_components):\n",
        "        super(MoGPrior, self).__init__()\n",
        "\n",
        "        self.L = L\n",
        "        self.num_components = num_components\n",
        "\n",
        "        # params\n",
        "        self.means = nn.Parameter(torch.randn(num_components, self.L))\n",
        "        self.logvars = nn.Parameter(torch.randn(num_components, self.L))\n",
        "\n",
        "        # mixing weights\n",
        "        self.w = nn.Parameter(torch.zeros(num_components, 1, 1))\n",
        "\n",
        "    def get_params(self):\n",
        "        return self.means, self.logvars\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        # mu, lof_var\n",
        "        means, logvars = self.get_params()\n",
        "\n",
        "        # mixing probabilities\n",
        "        w = F.softmax(self.w, dim=0)\n",
        "        w = w.squeeze()\n",
        "\n",
        "        # pick components\n",
        "        indexes = torch.multinomial(w, batch_size, replacement=True)\n",
        "\n",
        "        # means and logvars\n",
        "        eps = torch.randn(batch_size, self.L)\n",
        "        for i in range(batch_size):\n",
        "            indx = indexes[i]\n",
        "            m = means[[indx]].to('cuda')\n",
        "            e = eps[[i]].to('cuda')\n",
        "            l = torch.exp(logvars[[indx]]).to('cuda')\n",
        "            if i == 0:\n",
        "                z = m + e * l\n",
        "            else:\n",
        "                z = torch.cat((z, m + e * l), 0)\n",
        "        return z\n",
        "\n",
        "    def log_prob(self, z):\n",
        "        # mu, lof_var\n",
        "        means, logvars = self.get_params()\n",
        "\n",
        "        # mixing probabilities\n",
        "        w = F.softmax(self.w, dim=0)\n",
        "\n",
        "        # log-mixture-of-Gaussians\n",
        "        z = z.unsqueeze(0) # 1 x B x L\n",
        "        means = means.unsqueeze(1) # K x 1 x L\n",
        "        logvars = logvars.unsqueeze(1) # K x 1 x L\n",
        "\n",
        "        log_p = log_normal_diag(z, means, logvars) + torch.log(w) # K x B x L\n",
        "        log_prob = torch.logsumexp(log_p, dim=0, keepdim=False) # B x L\n",
        "\n",
        "        return log_prob"
      ],
      "metadata": {
        "id": "xQIvee5Cp69V",
        "execution": {
          "iopub.status.busy": "2024-06-07T14:40:18.255839Z",
          "iopub.execute_input": "2024-06-07T14:40:18.256094Z",
          "iopub.status.idle": "2024-06-07T14:40:18.268953Z",
          "shell.execute_reply.started": "2024-06-07T14:40:18.256072Z",
          "shell.execute_reply": "2024-06-07T14:40:18.268110Z"
        },
        "trusted": true
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Complete VAE\n",
        "\n",
        "The last class is `VAE` tha combines all components. Please remember that this class must implement the **Negative ELBO** in `forward`, as well as `sample` (*hint*: it is a composition of `sample` functions from the prior and the decoder)."
      ],
      "metadata": {
        "id": "aOM4QM9I_62d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE GOES HERE\n",
        "# This class combines Encoder, Decoder and Prior.\n",
        "# NOTES:\n",
        "# (i) The function \"sample\" must be implemented.\n",
        "# (ii) The function \"forward\" must return the negative ELBO. Please remember to add an argument \"reduction\" that is either \"mean\" or \"sum\".\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self, encoder_net, decoder_net, prior, num_vals=256, likelihood_type='categorical'):\n",
        "        super(VAE, self).__init__()\n",
        "\n",
        "        self.encoder = Encoder(encoder_net=encoder_net)\n",
        "        self.decoder = Decoder(distribution=likelihood_type, decoder_net=decoder_net, num_vals=num_vals)\n",
        "        self.prior = prior\n",
        "\n",
        "        self.num_vals = num_vals\n",
        "\n",
        "        self.likelihood_type = likelihood_type\n",
        "\n",
        "    def forward(self, x, reduction='mean'):\n",
        "        # encoder\n",
        "        mu_e, log_var_e = self.encoder.encode(x)\n",
        "        z = self.encoder.sample(mu_e=mu_e, log_var_e=log_var_e)\n",
        "\n",
        "        # ELBO\n",
        "        RE = self.decoder.log_prob(x, z)\n",
        "        KL = (self.prior.log_prob(z) - self.encoder.log_prob(mu_e=mu_e, log_var_e=log_var_e, z=z)).sum(-1)\n",
        "\n",
        "        if reduction == 'sum':\n",
        "            return -(RE + KL).sum()\n",
        "        else:\n",
        "            return -(RE + KL).mean()\n",
        "\n",
        "    def sample(self, batch_size=64):\n",
        "        z = self.prior.sample(batch_size=batch_size)\n",
        "        return self.decoder.sample(z)"
      ],
      "metadata": {
        "id": "OQpf-BeSqA6V",
        "execution": {
          "iopub.status.busy": "2024-06-07T14:40:18.301644Z",
          "iopub.execute_input": "2024-06-07T14:40:18.303140Z",
          "iopub.status.idle": "2024-06-07T14:40:18.314032Z",
          "shell.execute_reply.started": "2024-06-07T14:40:18.303099Z",
          "shell.execute_reply": "2024-06-07T14:40:18.313216Z"
        },
        "trusted": true
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation and training functions\n",
        "\n",
        "**Please DO NOT remove or modify them.**"
      ],
      "metadata": {
        "id": "hLhgze7DA4yx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DO NOT REMOVE\n",
        "def evaluation(test_loader, name=None, model_best=None, epoch=None):\n",
        "    # EVALUATION\n",
        "    if model_best is None:\n",
        "        # load best performing model\n",
        "        model_best = torch.load(name + '.model')\n",
        "\n",
        "    model_best.eval()\n",
        "    loss = 0.\n",
        "    N = 0.\n",
        "    for indx_batch, (test_batch, _) in enumerate(test_loader):\n",
        "        test_batch = test_batch.to(device)\n",
        "        loss_t = model_best.forward(test_batch, reduction='sum')\n",
        "        loss = loss + loss_t.item()\n",
        "        N = N + test_batch.shape[0]\n",
        "    loss = loss / N\n",
        "\n",
        "    if epoch is None:\n",
        "        print(f'FINAL LOSS: nll={loss}')\n",
        "    else:\n",
        "        print(f'Epoch: {epoch}, val nll={loss}')\n",
        "\n",
        "    return loss\n",
        "\n",
        "\n",
        "def samples_real(name, test_loader, shape=(28,28)):\n",
        "    # real images-------\n",
        "    num_x = 4\n",
        "    num_y = 4\n",
        "    x, _ = next(iter(test_loader))\n",
        "    x = x.to('cpu').detach().numpy()\n",
        "\n",
        "    fig, ax = plt.subplots(num_x, num_y)\n",
        "    for i, ax in enumerate(ax.flatten()):\n",
        "        plottable_image = np.reshape(x[i], shape)\n",
        "        ax.imshow(plottable_image, cmap='gray')\n",
        "        ax.axis('off')\n",
        "\n",
        "    plt.savefig(name+'_real_images.pdf', bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def samples_generated(name, data_loader, shape=(28,28), extra_name=''):\n",
        "    x, _ = next(iter(data_loader))\n",
        "    x = x.to('cpu').detach().numpy()\n",
        "\n",
        "    # generations-------\n",
        "    model_best = torch.load(name + '.model')\n",
        "    model_best.eval()\n",
        "\n",
        "    num_x = 4\n",
        "    num_y = 4\n",
        "    x = model_best.sample(num_x * num_y)\n",
        "    x = x.to('cpu').detach().numpy()\n",
        "\n",
        "    fig, ax = plt.subplots(num_x, num_y)\n",
        "    for i, ax in enumerate(ax.flatten()):\n",
        "        plottable_image = np.reshape(x[i], shape)\n",
        "        ax.imshow(plottable_image, cmap='gray')\n",
        "        ax.axis('off')\n",
        "\n",
        "    plt.savefig(name + '_generated_images' + extra_name + '.pdf', bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def plot_curve(name, nll_val):\n",
        "    plt.plot(np.arange(len(nll_val)), nll_val, linewidth='3')\n",
        "    plt.xlabel('epochs')\n",
        "    plt.ylabel('nll')\n",
        "    plt.savefig(name + '_nll_val_curve.pdf', bbox_inches='tight')\n",
        "    plt.close()"
      ],
      "metadata": {
        "id": "I9Dr3a6lqJ0W",
        "execution": {
          "iopub.status.busy": "2024-06-07T14:40:18.315017Z",
          "iopub.execute_input": "2024-06-07T14:40:18.315319Z",
          "iopub.status.idle": "2024-06-07T14:40:18.330484Z",
          "shell.execute_reply.started": "2024-06-07T14:40:18.315295Z",
          "shell.execute_reply": "2024-06-07T14:40:18.329670Z"
        },
        "trusted": true
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DO NOT REMOVE\n",
        "def training(name, max_patience, num_epochs, model, optimizer, training_loader, val_loader, shape=(28,28)):\n",
        "    nll_val = []\n",
        "    best_nll = 1000.\n",
        "    patience = 0\n",
        "\n",
        "    # Main loop\n",
        "    for e in range(num_epochs):\n",
        "        # TRAINING\n",
        "        model.train()\n",
        "        for indx_batch, (batch, _) in enumerate(training_loader):\n",
        "            batch = batch.to(device)\n",
        "            loss = model.forward(batch, reduction='mean')\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # Validation\n",
        "        loss_val = evaluation(val_loader, model_best=model, epoch=e)\n",
        "        nll_val.append(loss_val)  # save for plotting\n",
        "\n",
        "        if e == 0:\n",
        "            print('saved!')\n",
        "            torch.save(model, name + '.model')\n",
        "            best_nll = loss_val\n",
        "        else:\n",
        "            if loss_val < best_nll:\n",
        "                print('saved!')\n",
        "                torch.save(model, name + '.model')\n",
        "                best_nll = loss_val\n",
        "                patience = 0\n",
        "\n",
        "                samples_generated(name, val_loader, shape=shape, extra_name=\"_epoch_\" + str(e))\n",
        "            else:\n",
        "                patience = patience + 1\n",
        "\n",
        "        if patience > max_patience:\n",
        "            break\n",
        "\n",
        "    nll_val = np.asarray(nll_val)\n",
        "\n",
        "    return nll_val"
      ],
      "metadata": {
        "id": "9ABgMeG0qFAP",
        "execution": {
          "iopub.status.busy": "2024-06-07T14:40:18.331650Z",
          "iopub.execute_input": "2024-06-07T14:40:18.332388Z",
          "iopub.status.idle": "2024-06-07T14:40:18.344260Z",
          "shell.execute_reply.started": "2024-06-07T14:40:18.332356Z",
          "shell.execute_reply": "2024-06-07T14:40:18.343505Z"
        },
        "trusted": true
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup\n",
        "\n",
        "**NOTE: *Please comment your code! Especially if you introduce any new variables (e.g., hyperparameters).***\n",
        "\n",
        "In the following cells, we define `transforms` for the dataset. Next, we initialize the data, a directory for results and some fixed hyperparameters."
      ],
      "metadata": {
        "id": "kWr8N2u2qNTu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DO NOT REMOVE\n",
        "transforms_train = torchvision.transforms.Compose([torchvision.transforms.ToTensor(),\n",
        "                                             torchvision.transforms.Lambda(lambda x: torch.bernoulli(x)),\n",
        "                                             torchvision.transforms.Lambda(lambda x: torch.flatten(x,0)),\n",
        "                                             ],\n",
        "                                            )\n",
        "\n",
        "transforms_test = torchvision.transforms.Compose([torchvision.transforms.ToTensor(),\n",
        "                                             torchvision.transforms.Lambda(lambda x: torch.bernoulli(x)),\n",
        "                                             torchvision.transforms.Lambda(lambda x: torch.flatten(x,0)),\n",
        "                                             ],\n",
        "                                            )"
      ],
      "metadata": {
        "id": "bFTE5jtYpxDV",
        "execution": {
          "iopub.status.busy": "2024-06-07T14:40:18.345173Z",
          "iopub.execute_input": "2024-06-07T14:40:18.345430Z",
          "iopub.status.idle": "2024-06-07T14:40:18.357723Z",
          "shell.execute_reply.started": "2024-06-07T14:40:18.345407Z",
          "shell.execute_reply": "2024-06-07T14:40:18.356942Z"
        },
        "trusted": true
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Please do not modify the code in the next cell."
      ],
      "metadata": {
        "id": "8SDcOBbGCM8z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DO NOT REMOVE\n",
        "#-dataset\n",
        "dataset = MNIST('/files/', train=True, download=True,\n",
        "                      transform=transforms_train\n",
        "                )\n",
        "\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [50000, 10000], generator=torch.Generator().manual_seed(14))\n",
        "\n",
        "test_dataset = MNIST('/files/', train=False, download=True,\n",
        "                      transform=transforms_test\n",
        "                     )\n",
        "#-dataloaders\n",
        "batch_size = 32\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "#-hyperparams (please do not modify them for the final report)\n",
        "num_epochs = 1000 # max. number of epochs\n",
        "max_patience = 20 # an early stopping is used, if training doesn't improve for longer than 20 epochs, it is stopped"
      ],
      "metadata": {
        "id": "ua2VzAqqx3gU",
        "execution": {
          "iopub.status.busy": "2024-06-07T14:40:18.358820Z",
          "iopub.execute_input": "2024-06-07T14:40:18.359213Z",
          "iopub.status.idle": "2024-06-07T14:40:18.452790Z",
          "shell.execute_reply.started": "2024-06-07T14:40:18.359182Z",
          "shell.execute_reply": "2024-06-07T14:40:18.451817Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ea8c947-c63d-4b84-b2ce-020ddcbc3f0f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to /files/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:01<00:00, 5671612.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /files/MNIST/raw/train-images-idx3-ubyte.gz to /files/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to /files/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 1103128.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /files/MNIST/raw/train-labels-idx1-ubyte.gz to /files/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to /files/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 9451063.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /files/MNIST/raw/t10k-images-idx3-ubyte.gz to /files/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to /files/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 11140660.10it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /files/MNIST/raw/t10k-labels-idx1-ubyte.gz to /files/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# DO NOT REMOVE\n",
        "#-creating a dir for saving results\n",
        "name = 'vae'  # NOTE: if you run multiple experiments, you would overwrite results. Please modify this part if necessary.\n",
        "result_dir = images_dir + name + '/'\n",
        "if not(os.path.exists(result_dir)):\n",
        "  os.mkdir(result_dir)"
      ],
      "metadata": {
        "id": "yXuz71Aqbc2s",
        "execution": {
          "iopub.status.busy": "2024-06-07T14:40:18.453923Z",
          "iopub.execute_input": "2024-06-07T14:40:18.454206Z",
          "iopub.status.idle": "2024-06-07T14:40:18.459025Z",
          "shell.execute_reply.started": "2024-06-07T14:40:18.454182Z",
          "shell.execute_reply": "2024-06-07T14:40:18.458135Z"
        },
        "trusted": true
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the next cell, please initialize the model. Please remember about commenting your code!"
      ],
      "metadata": {
        "id": "kmKDXMI0B231"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "D = 784   # input dimension\n",
        "L = 20    # number of latents\n",
        "M = 512   # the number of neurons in scale (s) and translation (t) nets\n",
        "num_components = 4**2\n",
        "num_vals = 1"
      ],
      "metadata": {
        "id": "YKt5zfWDw9hb",
        "execution": {
          "iopub.status.busy": "2024-06-07T14:40:18.459952Z",
          "iopub.execute_input": "2024-06-07T14:40:18.460187Z",
          "iopub.status.idle": "2024-06-07T14:40:18.469143Z",
          "shell.execute_reply.started": "2024-06-07T14:40:18.460166Z",
          "shell.execute_reply": "2024-06-07T14:40:18.468266Z"
        },
        "trusted": true
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE COMES HERE:\n",
        "#\n",
        "# your code goes here\n",
        "#\n",
        "# use all necessary code to initialize your VAE\n",
        "likelihood_type = 'bernoulli'\n",
        "encoder_net = nn.Sequential(nn.Linear(D, M), nn.LeakyReLU(),\n",
        "                        nn.Linear(M, M), nn.LeakyReLU(),\n",
        "                        nn.Linear(M, 2 * L))\n",
        "\n",
        "decoder_net = nn.Sequential(nn.Linear(L, M), nn.LeakyReLU(),\n",
        "                        nn.Linear(M, M), nn.LeakyReLU(),\n",
        "                        nn.Linear(M, num_vals * D))\n",
        "\n",
        "\n",
        "prior = MoGPrior(L=L, num_components=num_components)\n",
        "\n",
        "model = VAE(encoder_net=encoder_net, decoder_net=decoder_net, num_vals=num_vals, prior=prior, likelihood_type=likelihood_type)\n",
        "model.to(device) # at the end, your model must be put on the available device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b73aaBDxqSYb",
        "outputId": "996cafaf-6a2e-4b16-8ca7-59688751d38f",
        "execution": {
          "iopub.status.busy": "2024-06-07T14:40:18.470051Z",
          "iopub.execute_input": "2024-06-07T14:40:18.470290Z",
          "iopub.status.idle": "2024-06-07T14:40:18.538074Z",
          "shell.execute_reply.started": "2024-06-07T14:40:18.470269Z",
          "shell.execute_reply": "2024-06-07T14:40:18.537138Z"
        },
        "trusted": true
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VAE(\n",
              "  (encoder): Encoder(\n",
              "    (encoder): Sequential(\n",
              "      (0): Linear(in_features=784, out_features=512, bias=True)\n",
              "      (1): LeakyReLU(negative_slope=0.01)\n",
              "      (2): Linear(in_features=512, out_features=512, bias=True)\n",
              "      (3): LeakyReLU(negative_slope=0.01)\n",
              "      (4): Linear(in_features=512, out_features=40, bias=True)\n",
              "    )\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (decoder): Sequential(\n",
              "      (0): Linear(in_features=20, out_features=512, bias=True)\n",
              "      (1): LeakyReLU(negative_slope=0.01)\n",
              "      (2): Linear(in_features=512, out_features=512, bias=True)\n",
              "      (3): LeakyReLU(negative_slope=0.01)\n",
              "      (4): Linear(in_features=512, out_features=784, bias=True)\n",
              "    )\n",
              "  )\n",
              "  (prior): MoGPrior()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Please initialize the optimizer"
      ],
      "metadata": {
        "id": "iC8AkWt4CURT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# PLEASE DEFINE YOUR OPTIMIZER\n",
        "#\n",
        "# your code goes here\n",
        "#\n",
        "# please do not forget to define hyperparameters!\n",
        "# please do it like this: optimizer = ...\n",
        "lr = 1e-3 # learning rate\n",
        "optimizer = torch.optim.Adamax([p for p in model.parameters() if p.requires_grad == True], lr=lr)"
      ],
      "metadata": {
        "id": "a3nTSDe7ql08",
        "execution": {
          "iopub.status.busy": "2024-06-07T14:40:18.539148Z",
          "iopub.execute_input": "2024-06-07T14:40:18.539431Z",
          "iopub.status.idle": "2024-06-07T14:40:18.544649Z",
          "shell.execute_reply.started": "2024-06-07T14:40:18.539405Z",
          "shell.execute_reply": "2024-06-07T14:40:18.543676Z"
        },
        "trusted": true
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training and final evaluation\n",
        "\n",
        "In the following two cells, we run the training and the final evaluation."
      ],
      "metadata": {
        "id": "P5GrzUcHFweG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DO NOT REMOVE OR MODIFY\n",
        "# Training procedure\n",
        "nll_val = training(name=result_dir + name, max_patience=max_patience,\n",
        "                   num_epochs=num_epochs, model=model, optimizer=optimizer,\n",
        "                   training_loader=train_loader, val_loader=val_loader,\n",
        "                   shape=(28,28))"
      ],
      "metadata": {
        "id": "VD7WuY6bqnBK",
        "execution": {
          "iopub.status.busy": "2024-06-07T14:40:18.545785Z",
          "iopub.execute_input": "2024-06-07T14:40:18.546066Z",
          "iopub.status.idle": "2024-06-07T15:53:10.210245Z",
          "shell.execute_reply.started": "2024-06-07T14:40:18.546043Z",
          "shell.execute_reply": "2024-06-07T15:53:10.209149Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee90917e-a157-4a4c-febc-660db6707a56"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, val nll=136.22712915039062\n",
            "saved!\n",
            "Epoch: 1, val nll=118.89757358398437\n",
            "saved!\n",
            "Epoch: 2, val nll=112.30618576660156\n",
            "saved!\n",
            "Epoch: 3, val nll=108.32340847167968\n",
            "saved!\n",
            "Epoch: 4, val nll=106.20068020019531\n",
            "saved!\n",
            "Epoch: 5, val nll=105.29122854003906\n",
            "saved!\n",
            "Epoch: 6, val nll=104.0980806640625\n",
            "saved!\n",
            "Epoch: 7, val nll=102.90478361816406\n",
            "saved!\n",
            "Epoch: 8, val nll=101.92579013671875\n",
            "saved!\n",
            "Epoch: 9, val nll=101.24797935791015\n",
            "saved!\n",
            "Epoch: 10, val nll=100.48193254394532\n",
            "saved!\n",
            "Epoch: 11, val nll=100.11855350341797\n",
            "saved!\n",
            "Epoch: 12, val nll=99.60178037109375\n",
            "saved!\n",
            "Epoch: 13, val nll=99.57480415039062\n",
            "saved!\n",
            "Epoch: 14, val nll=98.9441916015625\n",
            "saved!\n",
            "Epoch: 15, val nll=98.84942045898437\n",
            "saved!\n",
            "Epoch: 16, val nll=98.64430115966798\n",
            "saved!\n",
            "Epoch: 17, val nll=98.2411276123047\n",
            "saved!\n",
            "Epoch: 18, val nll=97.86161649169922\n",
            "saved!\n",
            "Epoch: 19, val nll=97.75957723388672\n",
            "saved!\n",
            "Epoch: 20, val nll=97.52694444580078\n",
            "saved!\n",
            "Epoch: 21, val nll=97.4272005859375\n",
            "saved!\n",
            "Epoch: 22, val nll=97.21818748779297\n",
            "saved!\n",
            "Epoch: 23, val nll=97.21202739257812\n",
            "saved!\n",
            "Epoch: 24, val nll=96.92877446289063\n",
            "saved!\n",
            "Epoch: 25, val nll=96.77051552734375\n",
            "saved!\n",
            "Epoch: 26, val nll=96.69094599609375\n",
            "saved!\n",
            "Epoch: 27, val nll=96.45913330078125\n",
            "saved!\n",
            "Epoch: 28, val nll=96.38415307617187\n",
            "saved!\n",
            "Epoch: 29, val nll=96.32852934570313\n",
            "saved!\n",
            "Epoch: 30, val nll=96.28379757080079\n",
            "saved!\n",
            "Epoch: 31, val nll=96.0977267578125\n",
            "saved!\n",
            "Epoch: 32, val nll=96.03513610839843\n",
            "saved!\n",
            "Epoch: 33, val nll=95.7382482421875\n",
            "saved!\n",
            "Epoch: 34, val nll=95.83349442138672\n",
            "Epoch: 35, val nll=95.97581618652343\n",
            "Epoch: 36, val nll=95.77866435546875\n",
            "Epoch: 37, val nll=95.46272937011719\n",
            "saved!\n",
            "Epoch: 38, val nll=95.56286057128906\n",
            "Epoch: 39, val nll=95.44483043212891\n",
            "saved!\n",
            "Epoch: 40, val nll=95.33018823242188\n",
            "saved!\n",
            "Epoch: 41, val nll=95.12725671386718\n",
            "saved!\n",
            "Epoch: 42, val nll=95.32328291015625\n",
            "Epoch: 43, val nll=95.03860938720703\n",
            "saved!\n",
            "Epoch: 44, val nll=94.946465234375\n",
            "saved!\n",
            "Epoch: 45, val nll=95.06247263183593\n",
            "Epoch: 46, val nll=95.02245446777344\n",
            "Epoch: 47, val nll=94.97416939697266\n",
            "Epoch: 48, val nll=94.99637747802734\n",
            "Epoch: 49, val nll=94.66901568603515\n",
            "saved!\n",
            "Epoch: 50, val nll=94.8414017578125\n",
            "Epoch: 51, val nll=94.82152692871094\n",
            "Epoch: 52, val nll=94.69859262695313\n",
            "Epoch: 53, val nll=94.80602648925782\n",
            "Epoch: 54, val nll=94.69135776367187\n",
            "Epoch: 55, val nll=94.76925148925781\n",
            "Epoch: 56, val nll=94.69232622070312\n",
            "Epoch: 57, val nll=94.4224361694336\n",
            "saved!\n",
            "Epoch: 58, val nll=94.6125564453125\n",
            "Epoch: 59, val nll=94.36722475585937\n",
            "saved!\n",
            "Epoch: 60, val nll=94.59014614257812\n",
            "Epoch: 61, val nll=94.41292021484375\n",
            "Epoch: 62, val nll=94.45935026855469\n",
            "Epoch: 63, val nll=94.33792841796875\n",
            "saved!\n",
            "Epoch: 64, val nll=94.60275755615234\n",
            "Epoch: 65, val nll=94.19123192138672\n",
            "saved!\n",
            "Epoch: 66, val nll=94.19753533935547\n",
            "Epoch: 67, val nll=94.4374841796875\n",
            "Epoch: 68, val nll=94.19954006347656\n",
            "Epoch: 69, val nll=94.21720297851563\n",
            "Epoch: 70, val nll=94.29729348144531\n",
            "Epoch: 71, val nll=94.18861030273438\n",
            "saved!\n",
            "Epoch: 72, val nll=94.07766645507813\n",
            "saved!\n",
            "Epoch: 73, val nll=94.2507085571289\n",
            "Epoch: 74, val nll=94.0691675415039\n",
            "saved!\n",
            "Epoch: 75, val nll=93.98745373535156\n",
            "saved!\n",
            "Epoch: 76, val nll=94.04806318359375\n",
            "Epoch: 77, val nll=94.3277517578125\n",
            "Epoch: 78, val nll=94.02956649169921\n",
            "Epoch: 79, val nll=93.95555219726562\n",
            "saved!\n",
            "Epoch: 80, val nll=94.06584587402344\n",
            "Epoch: 81, val nll=93.7889764892578\n",
            "saved!\n",
            "Epoch: 82, val nll=93.7920135498047\n",
            "Epoch: 83, val nll=93.92877852783204\n",
            "Epoch: 84, val nll=93.92476707763672\n",
            "Epoch: 85, val nll=93.924894140625\n",
            "Epoch: 86, val nll=93.81708383789062\n",
            "Epoch: 87, val nll=93.81982590332031\n",
            "Epoch: 88, val nll=93.83792840576172\n",
            "Epoch: 89, val nll=93.8274669921875\n",
            "Epoch: 90, val nll=93.70640607910157\n",
            "saved!\n",
            "Epoch: 91, val nll=93.8373396484375\n",
            "Epoch: 92, val nll=93.63544365234375\n",
            "saved!\n",
            "Epoch: 93, val nll=93.69491361083985\n",
            "Epoch: 94, val nll=93.85424990234375\n",
            "Epoch: 95, val nll=93.87992305908203\n",
            "Epoch: 96, val nll=93.68111755371093\n",
            "Epoch: 97, val nll=93.68258256835938\n",
            "Epoch: 98, val nll=93.97491312255859\n",
            "Epoch: 99, val nll=93.67760266113281\n",
            "Epoch: 100, val nll=93.92765124511719\n",
            "Epoch: 101, val nll=93.70794475097657\n",
            "Epoch: 102, val nll=93.49304113769531\n",
            "saved!\n",
            "Epoch: 103, val nll=93.54742432861327\n",
            "Epoch: 104, val nll=93.65450552978515\n",
            "Epoch: 105, val nll=93.64162316894532\n",
            "Epoch: 106, val nll=93.84637432861328\n",
            "Epoch: 107, val nll=93.56457305908204\n",
            "Epoch: 108, val nll=93.51124129638671\n",
            "Epoch: 109, val nll=93.43034897460937\n",
            "saved!\n",
            "Epoch: 110, val nll=93.51706768798829\n",
            "Epoch: 111, val nll=93.66370290527344\n",
            "Epoch: 112, val nll=93.41389353027344\n",
            "saved!\n",
            "Epoch: 113, val nll=93.31262413330079\n",
            "saved!\n",
            "Epoch: 114, val nll=93.35690417480468\n",
            "Epoch: 115, val nll=93.41575769042969\n",
            "Epoch: 116, val nll=93.52862209472656\n",
            "Epoch: 117, val nll=93.59967331542968\n",
            "Epoch: 118, val nll=93.30044714355469\n",
            "saved!\n",
            "Epoch: 119, val nll=93.39564211425781\n",
            "Epoch: 120, val nll=93.58698195800781\n",
            "Epoch: 121, val nll=93.45888344726562\n",
            "Epoch: 122, val nll=93.41119526367187\n",
            "Epoch: 123, val nll=93.52851477050781\n",
            "Epoch: 124, val nll=93.2774072998047\n",
            "saved!\n",
            "Epoch: 125, val nll=93.40507954101562\n",
            "Epoch: 126, val nll=93.41468138427734\n",
            "Epoch: 127, val nll=93.22272309570313\n",
            "saved!\n",
            "Epoch: 128, val nll=93.26551662597656\n",
            "Epoch: 129, val nll=93.32451364746093\n",
            "Epoch: 130, val nll=93.38544338378907\n",
            "Epoch: 131, val nll=93.5093216796875\n",
            "Epoch: 132, val nll=93.14299592285157\n",
            "saved!\n",
            "Epoch: 133, val nll=93.17457634277343\n",
            "Epoch: 134, val nll=93.45598479003907\n",
            "Epoch: 135, val nll=93.2289825439453\n",
            "Epoch: 136, val nll=93.33199881591797\n",
            "Epoch: 137, val nll=93.19651954345703\n",
            "Epoch: 138, val nll=93.38320771484375\n",
            "Epoch: 139, val nll=93.34191857910156\n",
            "Epoch: 140, val nll=93.3964254272461\n",
            "Epoch: 141, val nll=93.21389582519531\n",
            "Epoch: 142, val nll=93.25770588378906\n",
            "Epoch: 143, val nll=93.27648646240235\n",
            "Epoch: 144, val nll=93.15349606933594\n",
            "Epoch: 145, val nll=93.18952172851563\n",
            "Epoch: 146, val nll=93.14485070800781\n",
            "Epoch: 147, val nll=93.26552427978515\n",
            "Epoch: 148, val nll=93.10715634765624\n",
            "saved!\n",
            "Epoch: 149, val nll=92.96568483886719\n",
            "saved!\n",
            "Epoch: 150, val nll=93.05527955322266\n",
            "Epoch: 151, val nll=92.97611828613282\n",
            "Epoch: 152, val nll=93.0188080444336\n",
            "Epoch: 153, val nll=93.1917255859375\n",
            "Epoch: 154, val nll=93.15564418945313\n",
            "Epoch: 155, val nll=93.15925522460938\n",
            "Epoch: 156, val nll=93.40070661621094\n",
            "Epoch: 157, val nll=93.1253680053711\n",
            "Epoch: 158, val nll=93.26655\n",
            "Epoch: 159, val nll=92.98393002929687\n",
            "Epoch: 160, val nll=93.07308645019532\n",
            "Epoch: 161, val nll=93.05790886230469\n",
            "Epoch: 162, val nll=93.24465412597657\n",
            "Epoch: 163, val nll=92.95796232910156\n",
            "saved!\n",
            "Epoch: 164, val nll=93.15143927001954\n",
            "Epoch: 165, val nll=93.1824780883789\n",
            "Epoch: 166, val nll=93.2431880859375\n",
            "Epoch: 167, val nll=92.91211311035157\n",
            "saved!\n",
            "Epoch: 168, val nll=93.01531837158203\n",
            "Epoch: 169, val nll=93.11647495117188\n",
            "Epoch: 170, val nll=93.11807059326172\n",
            "Epoch: 171, val nll=93.27082873535156\n",
            "Epoch: 172, val nll=93.02842084960938\n",
            "Epoch: 173, val nll=92.96126148681641\n",
            "Epoch: 174, val nll=92.98899345703126\n",
            "Epoch: 175, val nll=92.9401083618164\n",
            "Epoch: 176, val nll=93.06669404296875\n",
            "Epoch: 177, val nll=92.98394760742187\n",
            "Epoch: 178, val nll=93.02341557617187\n",
            "Epoch: 179, val nll=93.0095888671875\n",
            "Epoch: 180, val nll=93.05051221923829\n",
            "Epoch: 181, val nll=92.91833837890626\n",
            "Epoch: 182, val nll=93.07009827880859\n",
            "Epoch: 183, val nll=92.99343010253907\n",
            "Epoch: 184, val nll=92.95706826171875\n",
            "Epoch: 185, val nll=93.1467406616211\n",
            "Epoch: 186, val nll=92.90333942871094\n",
            "saved!\n",
            "Epoch: 187, val nll=93.16017775878906\n",
            "Epoch: 188, val nll=93.18280726318359\n",
            "Epoch: 189, val nll=92.96163032226562\n",
            "Epoch: 190, val nll=93.01136320800781\n",
            "Epoch: 191, val nll=93.0939682373047\n",
            "Epoch: 192, val nll=92.89777822265626\n",
            "saved!\n",
            "Epoch: 193, val nll=92.99762689208984\n",
            "Epoch: 194, val nll=92.9800946044922\n",
            "Epoch: 195, val nll=93.01714018554688\n",
            "Epoch: 196, val nll=93.07542504882812\n",
            "Epoch: 197, val nll=92.9538276611328\n",
            "Epoch: 198, val nll=93.03329599609376\n",
            "Epoch: 199, val nll=93.0290837158203\n",
            "Epoch: 200, val nll=93.08303654785156\n",
            "Epoch: 201, val nll=92.85973649902344\n",
            "saved!\n",
            "Epoch: 202, val nll=92.78098961181641\n",
            "saved!\n",
            "Epoch: 203, val nll=93.13817906494141\n",
            "Epoch: 204, val nll=92.672821484375\n",
            "saved!\n",
            "Epoch: 205, val nll=92.86706391601562\n",
            "Epoch: 206, val nll=93.00426785888672\n",
            "Epoch: 207, val nll=92.90982799072266\n",
            "Epoch: 208, val nll=93.04589033203125\n",
            "Epoch: 209, val nll=92.87262416992188\n",
            "Epoch: 210, val nll=92.7618283935547\n",
            "Epoch: 211, val nll=92.97749099121094\n",
            "Epoch: 212, val nll=92.64490285644531\n",
            "saved!\n",
            "Epoch: 213, val nll=93.05399262695312\n",
            "Epoch: 214, val nll=92.7639626953125\n",
            "Epoch: 215, val nll=92.97313839111328\n",
            "Epoch: 216, val nll=92.8092326171875\n",
            "Epoch: 217, val nll=92.96266805419921\n",
            "Epoch: 218, val nll=92.84726219482422\n",
            "Epoch: 219, val nll=92.66498216552735\n",
            "Epoch: 220, val nll=92.98651149902344\n",
            "Epoch: 221, val nll=92.81290385742187\n",
            "Epoch: 222, val nll=92.82064509277343\n",
            "Epoch: 223, val nll=92.77596130371094\n",
            "Epoch: 224, val nll=92.83512033691406\n",
            "Epoch: 225, val nll=92.84626785888672\n",
            "Epoch: 226, val nll=92.86288955078125\n",
            "Epoch: 227, val nll=92.76528669433594\n",
            "Epoch: 228, val nll=92.87746842041015\n",
            "Epoch: 229, val nll=92.96427863769532\n",
            "Epoch: 230, val nll=92.83590184326172\n",
            "Epoch: 231, val nll=92.72312021484375\n",
            "Epoch: 232, val nll=92.79731322021485\n",
            "Epoch: 233, val nll=92.85455478515625\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# DO NOT REMOVE OR MODIFY\n",
        "# Final evaluation\n",
        "test_loss = evaluation(name=result_dir + name, test_loader=test_loader)\n",
        "f = open(result_dir + name + '_test_loss.txt', \"w\")\n",
        "f.write(str(test_loss))\n",
        "f.close()\n",
        "\n",
        "samples_real(result_dir + name, test_loader)\n",
        "samples_generated(result_dir + name, test_loader, extra_name='_FINAL')\n",
        "\n",
        "plot_curve(result_dir + name, nll_val)"
      ],
      "metadata": {
        "id": "JAuMt9_wquOI",
        "execution": {
          "iopub.status.busy": "2024-06-07T15:53:10.211571Z",
          "iopub.execute_input": "2024-06-07T15:53:10.212183Z",
          "iopub.status.idle": "2024-06-07T15:53:13.733572Z",
          "shell.execute_reply.started": "2024-06-07T15:53:10.212148Z",
          "shell.execute_reply": "2024-06-07T15:53:13.732107Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3176aff-74e8-4d1e-a568-a606825fcbd3"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FINAL LOSS: nll=92.0731965576172\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "name = 'vae_1'  # NOTE: if you run multiple experiments, you would overwrite results. Please modify this part if necessary.\n",
        "result_dir = images_dir + name + '/'\n",
        "if not(os.path.exists(result_dir)):\n",
        "  os.mkdir(result_dir)"
      ],
      "metadata": {
        "id": "k_e0CAXkwsz4"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr = 1e-5 # learning rate\n",
        "optimizer = torch.optim.Adamax([p for p in model.parameters() if p.requires_grad == True], lr=lr)"
      ],
      "metadata": {
        "id": "FQPqL-KZw76y"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DO NOT REMOVE OR MODIFY\n",
        "# Training procedure\n",
        "nll_val = training(name=result_dir + name, max_patience=max_patience,\n",
        "                   num_epochs=num_epochs, model=model, optimizer=optimizer,\n",
        "                   training_loader=train_loader, val_loader=val_loader,\n",
        "                   shape=(28,28))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3xzfpFocxQpj",
        "outputId": "9c9c1a70-5924-4121-9695-6eb7d9308732"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, val nll=91.97083553466797\n",
            "saved!\n",
            "Epoch: 1, val nll=91.8662556640625\n",
            "saved!\n",
            "Epoch: 2, val nll=91.8257950439453\n",
            "saved!\n",
            "Epoch: 3, val nll=91.72795203857422\n",
            "saved!\n",
            "Epoch: 4, val nll=91.71311966552734\n",
            "saved!\n",
            "Epoch: 5, val nll=91.61728342285156\n",
            "saved!\n",
            "Epoch: 6, val nll=91.64439987792969\n",
            "Epoch: 7, val nll=91.61350727539063\n",
            "saved!\n",
            "Epoch: 8, val nll=91.39154611816406\n",
            "saved!\n",
            "Epoch: 9, val nll=91.58637529296875\n",
            "Epoch: 10, val nll=91.46661030273438\n",
            "Epoch: 11, val nll=91.53690633544922\n",
            "Epoch: 12, val nll=91.50001030273438\n",
            "Epoch: 13, val nll=91.40628432617187\n",
            "Epoch: 14, val nll=91.31596394042968\n",
            "saved!\n",
            "Epoch: 15, val nll=91.33376359863281\n",
            "Epoch: 16, val nll=91.44355427246094\n",
            "Epoch: 17, val nll=91.35253356933593\n",
            "Epoch: 18, val nll=91.40220198974609\n",
            "Epoch: 19, val nll=91.36551717529296\n",
            "Epoch: 20, val nll=91.43068719482422\n",
            "Epoch: 21, val nll=91.43219829101562\n",
            "Epoch: 22, val nll=91.36784246826171\n",
            "Epoch: 23, val nll=91.37155146484375\n",
            "Epoch: 24, val nll=91.34352165527343\n",
            "Epoch: 25, val nll=91.28199445800782\n",
            "saved!\n",
            "Epoch: 26, val nll=91.36103720703125\n",
            "Epoch: 27, val nll=91.13174916992187\n",
            "saved!\n",
            "Epoch: 28, val nll=91.3295600830078\n",
            "Epoch: 29, val nll=91.30851206054687\n",
            "Epoch: 30, val nll=91.26377145996094\n",
            "Epoch: 31, val nll=91.29760091552734\n",
            "Epoch: 32, val nll=91.16598237304687\n",
            "Epoch: 33, val nll=91.26486640625\n",
            "Epoch: 34, val nll=91.17062062988282\n",
            "Epoch: 35, val nll=91.34162229003907\n",
            "Epoch: 36, val nll=91.23564255371093\n",
            "Epoch: 37, val nll=91.22489089355469\n",
            "Epoch: 38, val nll=91.27617413330078\n",
            "Epoch: 39, val nll=91.21214045410156\n",
            "Epoch: 40, val nll=91.0965976928711\n",
            "saved!\n",
            "Epoch: 41, val nll=91.20855544433594\n",
            "Epoch: 42, val nll=91.31338454589844\n",
            "Epoch: 43, val nll=91.32044990234375\n",
            "Epoch: 44, val nll=91.17288608398438\n",
            "Epoch: 45, val nll=91.23416015625\n",
            "Epoch: 46, val nll=91.14062868652344\n",
            "Epoch: 47, val nll=91.20788034667969\n",
            "Epoch: 48, val nll=91.20143408203126\n",
            "Epoch: 49, val nll=91.1371470703125\n",
            "Epoch: 50, val nll=91.36473098144532\n",
            "Epoch: 51, val nll=91.22502274169922\n",
            "Epoch: 52, val nll=91.1351104248047\n",
            "Epoch: 53, val nll=91.23620432128907\n",
            "Epoch: 54, val nll=91.03907697753907\n",
            "saved!\n",
            "Epoch: 55, val nll=91.14028815917969\n",
            "Epoch: 56, val nll=91.20161325683594\n",
            "Epoch: 57, val nll=91.28145388183594\n",
            "Epoch: 58, val nll=91.24274758300781\n",
            "Epoch: 59, val nll=91.21219820556641\n",
            "Epoch: 60, val nll=91.22256313476562\n",
            "Epoch: 61, val nll=91.24345792236328\n",
            "Epoch: 62, val nll=91.13487758789063\n",
            "Epoch: 63, val nll=91.22903377685547\n",
            "Epoch: 64, val nll=91.14364812011719\n",
            "Epoch: 65, val nll=91.25766579589843\n",
            "Epoch: 66, val nll=91.19048267822265\n",
            "Epoch: 67, val nll=91.16980279541016\n",
            "Epoch: 68, val nll=91.27673845214844\n",
            "Epoch: 69, val nll=91.22294045410156\n",
            "Epoch: 70, val nll=91.215205078125\n",
            "Epoch: 71, val nll=91.18236140136719\n",
            "Epoch: 72, val nll=91.17227536621094\n",
            "Epoch: 73, val nll=91.24143781738282\n",
            "Epoch: 74, val nll=91.09915158691406\n",
            "Epoch: 75, val nll=91.10447690429687\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# DO NOT REMOVE OR MODIFY\n",
        "# Final evaluation\n",
        "test_loss = evaluation(name=result_dir + name, test_loader=test_loader)\n",
        "f = open(result_dir + name + '_test_loss.txt', \"w\")\n",
        "f.write(str(test_loss))\n",
        "f.close()\n",
        "\n",
        "samples_real(result_dir + name, test_loader)\n",
        "samples_generated(result_dir + name, test_loader, extra_name='_FINAL')\n",
        "\n",
        "plot_curve(result_dir + name, nll_val)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_mfQfTatxUG3",
        "outputId": "c1275c0b-8d0d-475e-ba68-98c653cd19f2"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FINAL LOSS: nll=90.60689251708985\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "name = 'vae_2'  # NOTE: if you run multiple experiments, you would overwrite results. Please modify this part if necessary.\n",
        "result_dir = images_dir + name + '/'\n",
        "if not(os.path.exists(result_dir)):\n",
        "  os.mkdir(result_dir)"
      ],
      "metadata": {
        "id": "tNFmXlzswvIe"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr = 1e-2 # learning rate\n",
        "optimizer = torch.optim.Adamax([p for p in model.parameters() if p.requires_grad == True], lr=lr)"
      ],
      "metadata": {
        "id": "KJFoQ04kw8s3"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DO NOT REMOVE OR MODIFY\n",
        "# Training procedure\n",
        "nll_val = training(name=result_dir + name, max_patience=max_patience,\n",
        "                   num_epochs=num_epochs, model=model, optimizer=optimizer,\n",
        "                   training_loader=train_loader, val_loader=val_loader,\n",
        "                   shape=(28,28))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9gMCHAzfxR7_",
        "outputId": "ae103cbd-ad66-4ea7-bbad-46239c47e338"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, val nll=105.14024040527343\n",
            "saved!\n",
            "Epoch: 1, val nll=103.6465234008789\n",
            "saved!\n",
            "Epoch: 2, val nll=102.91070706787109\n",
            "saved!\n",
            "Epoch: 3, val nll=102.7950892578125\n",
            "saved!\n",
            "Epoch: 4, val nll=102.70966417236328\n",
            "saved!\n",
            "Epoch: 5, val nll=102.12340202636719\n",
            "saved!\n",
            "Epoch: 6, val nll=102.006349609375\n",
            "saved!\n",
            "Epoch: 7, val nll=101.45420549316407\n",
            "saved!\n",
            "Epoch: 8, val nll=101.02888596191406\n",
            "saved!\n",
            "Epoch: 9, val nll=101.89416740722656\n",
            "Epoch: 10, val nll=102.55333774414062\n",
            "Epoch: 11, val nll=100.94116365966796\n",
            "saved!\n",
            "Epoch: 12, val nll=101.04043247070312\n",
            "Epoch: 13, val nll=101.78677746582031\n",
            "Epoch: 14, val nll=101.76523598632812\n",
            "Epoch: 15, val nll=101.60910043945313\n",
            "Epoch: 16, val nll=101.14382888183594\n",
            "Epoch: 17, val nll=101.00662221679687\n",
            "Epoch: 18, val nll=100.70915847167969\n",
            "saved!\n",
            "Epoch: 19, val nll=100.99990473632812\n",
            "Epoch: 20, val nll=100.97823903808593\n",
            "Epoch: 21, val nll=101.99442116699218\n",
            "Epoch: 22, val nll=100.2013169921875\n",
            "saved!\n",
            "Epoch: 23, val nll=100.95961561279297\n",
            "Epoch: 24, val nll=101.80898258056641\n",
            "Epoch: 25, val nll=101.66983516845703\n",
            "Epoch: 26, val nll=100.30217202148438\n",
            "Epoch: 27, val nll=101.46755615234375\n",
            "Epoch: 28, val nll=101.63673527832032\n",
            "Epoch: 29, val nll=102.52888911132813\n",
            "Epoch: 30, val nll=100.7850319091797\n",
            "Epoch: 31, val nll=101.3224459350586\n",
            "Epoch: 32, val nll=103.45420645751953\n",
            "Epoch: 33, val nll=102.00987593994141\n",
            "Epoch: 34, val nll=101.8609646484375\n",
            "Epoch: 35, val nll=100.38717053222656\n",
            "Epoch: 36, val nll=100.80033347167969\n",
            "Epoch: 37, val nll=100.86064267578125\n",
            "Epoch: 38, val nll=101.60401689453126\n",
            "Epoch: 39, val nll=100.37626982421875\n",
            "Epoch: 40, val nll=100.90266672363282\n",
            "Epoch: 41, val nll=107.64275694580078\n",
            "Epoch: 42, val nll=101.91774700927735\n",
            "Epoch: 43, val nll=219.89041711425782\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# DO NOT REMOVE OR MODIFY\n",
        "# Final evaluation\n",
        "test_loss = evaluation(name=result_dir + name, test_loader=test_loader)\n",
        "f = open(result_dir + name + '_test_loss.txt', \"w\")\n",
        "f.write(str(test_loss))\n",
        "f.close()\n",
        "\n",
        "samples_real(result_dir + name, test_loader)\n",
        "samples_generated(result_dir + name, test_loader, extra_name='_FINAL')\n",
        "\n",
        "plot_curve(result_dir + name, nll_val)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pjbmLod7xWfM",
        "outputId": "866d6973-d038-40f1-d5bf-26b510f4a7df"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FINAL LOSS: nll=99.7722010986328\n"
          ]
        }
      ]
    }
  ]
}